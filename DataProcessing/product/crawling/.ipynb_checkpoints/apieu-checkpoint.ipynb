{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs              # 데이터파싱 라이브러리\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import json\n",
    "import platform\n",
    "import re\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chromedriver.exe' if (platform.system() == 'Windows') else '/Users/jg/Desktop/develop/DataTeam/DataProcessing/product/crawling/chromedriver';\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJSON(jsonString, output_name='data.json'):\n",
    "    with open(output_name,'w',encoding='UTF-8') as file:\n",
    "        file.write(jsonString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorys = [115015010000]\n",
    "categorys = [115015007000, 115015008000, 115015017000, 115015015000, 115015014000, 115015009000, 115015010000, 120000000328, 120000000333, 115015013000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrlList():\n",
    "    urlList = []\n",
    "    for category in categorys:\n",
    "        #카테고리별로 접속\n",
    "        driver.get(\"http://apieu.beautynet.co.kr/goods.list.do?upperDisplayCategoryNumber=\" + str(category) + \"&displayCategoryNumber=0\")\n",
    "        \n",
    "        #상품 긁기\n",
    "        while True:\n",
    "            html = driver.page_source\n",
    "            soup = bs(html,'html.parser')\n",
    "            products = driver.find_elements_by_xpath(\"//p[@class='pname']//a\")\n",
    "            for product in products:\n",
    "                urlList.append(product.get_attribute(\"href\"))\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//a[@class='nextBtn']\").click()\n",
    "            except NoSuchElementException:\n",
    "                break;\n",
    "    return urlList\n",
    "        \n",
    "        #다음 페이지가 있으면 다음 페이지로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlList = getUrlList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n"
     ]
    }
   ],
   "source": [
    "urlList = list(set(urlList))\n",
    "print(len(urlList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItemDetailByUrl(urlList):\n",
    "    result_json = []\n",
    "    for idx, item in enumerate(urlList):\n",
    "        driver.get(item)\n",
    "        html = driver.page_source\n",
    "        soup = bs(html,'html.parser')\n",
    "        # dictionary 생성\n",
    "        result = {'name':'', 'url':'', 'image':'', 'salePrice':'', 'originalPrice':'', 'color':'', \n",
    "                   'category':'', 'brand':'어퓨','volume':''}\n",
    "        \n",
    "        fullName = soup.find('h2',{'id':'goodsProdName'}).get_text().strip('[어퓨]')\n",
    "        checkContainsColor = fullName.find('[')\n",
    "        if checkContainsColor == -1:\n",
    "            result['name'] = fullName\n",
    "            result['color'] = \"#\"\n",
    "        else:\n",
    "            result['name'] = fullName[:checkContainsColor]\n",
    "            result['color'] = fullName[checkContainsColor + 1: len(fullName)]\n",
    "        \n",
    "        result['url'] = driver.current_url\n",
    "        categorys = []\n",
    "        findCategory = soup.find('div',{'class':'tit_path'}).find_all('li')\n",
    "        for category in findCategory:\n",
    "            categorys.append(category.get_text().strip('\\n'))\n",
    "        result['category'] = '>'.join(categorys)\n",
    "        \n",
    "        #컬러가 있는지 체크\n",
    "#         if soup.find('div', {'class':'colorList'}) is None:\n",
    "            #단품\n",
    "        images = driver.find_elements_by_xpath(\"//ul[@class='simg']//li//a//img\")\n",
    "        result['image'] = [image.get_attribute('src') for image in images]\n",
    "        result['salePrice'] = soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'price'}).get_text()\n",
    "        result['originalPrice'] = result['salePrice'] if soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'fixedPrice'}) is None else soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'fixedPrice'}).get_text()\n",
    "        infoOptions = soup.find('div',{'class':'itemBox dtlInfo'}).find_all('dl')\n",
    "        for option in infoOptions:\n",
    "            if option.dt.get_text() == \"용량\":\n",
    "                result['volume'] = option.dd.get_text()\n",
    "                break;\n",
    "        \n",
    "        #display(result)\n",
    "        result_json.append(result)\n",
    "#         else:\n",
    "#             #컬러 있음\n",
    "#             #Check # of Colors\n",
    "#             colorList = driver.find_elements_by_xpath(\"//div[@class='bx-viewport']//li\")\n",
    "#             for idx, color in enumerate(colorList):\n",
    "#                 color.click()\n",
    "#                 result_dict = copy.deepcopy(result)\n",
    "#                 images = driver.find_elements_by_xpath(\"//ul[@class='simg']//li//a//img\")\n",
    "#                 result_dict['image'] = [image.get_attribute('src') for image in images]\n",
    "#                 result_dict['salePrice'] = soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'price'}).get_text()\n",
    "#                 result_dict['originalPrice'] = result_dict['salePrice'] if soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'fixedPrice'}) is None else soup.find('div',{'class':'itemBox priceInfo'}).find('span',{'class':'fixedPrice'}).get_text()\n",
    "#                 infoOptions = soup.find('div',{'class':'itemBox dtlInfo'}).find_all('dl')\n",
    "#                 for option in infoOptions:\n",
    "#                     if option.dt.get_text() == \"용량\":\n",
    "#                         result_dict['volume'] = option.dd.get_text()\n",
    "#                         break;\n",
    "#                 result_dict['color'] = color.find_element_by_tag_name('a').get_attribute('data-option_goods_name')\n",
    "#                 result_json.append(result_dict)\n",
    "#                 display(result_dict)\n",
    "        if idx % (len(urlList)//50) == 0:\n",
    "            print(\"%3.1f 퍼센트 진행중\" % round(idx / len(urlList) * 100))\n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 퍼센트 진행중\n",
      "2.0 퍼센트 진행중\n"
     ]
    }
   ],
   "source": [
    "getItemDetailByUrl(urlList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
