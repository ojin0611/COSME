{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eglips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs              # 데이터파싱 라이브러리\n",
    "from selenium import webdriver\n",
    "import json\n",
    "import platform\n",
    "import re\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJSON(jsonString, output_name='data.json'):\n",
    "    with open(output_name,'w',encoding='UTF-8') as file:\n",
    "        file.write(jsonString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumber(string):\n",
    "    numExtracter = re.compile('[0-9]+')\n",
    "    return int(''.join(numExtracter.findall(string)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlingStart(driver, url_home):\n",
    "    html = driver.page_source\n",
    "    soup = bs(html,'html.parser')\n",
    "    url_items = soup.find_all('div',{'class':'thumb'})\n",
    "    url_page = soup.find('ol',{'class':'paging'}).find('a')['href']\n",
    "    page_numbers = len(soup.find('ol',{'class':'paging'}).find_all('li'))\n",
    "\n",
    "    result_json=[]\n",
    "    for page in range(1,page_numbers):\n",
    "        url = url_home+url_page[:-1]+str(page)\n",
    "        print(page, 'page crawling start!')\n",
    "        driver.get(url)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = bs(html,'html.parser')\n",
    "        url_items = soup.find_all('div',{'class':'thumb'})\n",
    "\n",
    "        for url_item in url_items:\n",
    "            url = url_home + url_item.a['href']\n",
    "            driver.get(url)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = bs(html,'html.parser')\n",
    "\n",
    "            name = soup.find('div',{'class':'info'}).h3.get_text().strip()\n",
    "\n",
    "            salePrice = soup.find('span',{'id':'pricevalue'})\n",
    "            salePrice = getNumber(salePrice.get_text()) if salePrice else ''\n",
    "\n",
    "            originalPrice = soup.find('strike')\n",
    "            originalPrice = getNumber(originalPrice.get_text()) if originalPrice else salePrice\n",
    "\n",
    "            image = url_home + soup.find('img',{'class':'detail_image'})['src']\n",
    "\n",
    "            tblefts = soup.find_all('div',{'class':'tb-left'})\n",
    "            volume_switch = False\n",
    "            for tbleft in tblefts:\n",
    "                if volume_switch:\n",
    "                    volume = tbleft.get_text().strip()\n",
    "                    volume_switch = False\n",
    "                if '특이사항' in tbleft:\n",
    "                    volume_switch = True\n",
    "                    \n",
    "            result = {'name':'#', 'url':'#', 'image':'#', 'salePrice':'#', 'originalPrice':'#', \n",
    "                      'color':'#', 'category':'#', 'brand':'EGLIPS','volume':'#'}\n",
    "\n",
    "            result['name']=name\n",
    "            result['salePrice']=salePrice\n",
    "            result['originalPrice']=originalPrice\n",
    "            result['image']=image\n",
    "            result['volume']=volume\n",
    "            result['url']=driver.current_url\n",
    "\n",
    "            colors = soup.find_all('option')\n",
    "            for color in colors:\n",
    "\n",
    "                if '--옵션 선택--' not in color:\n",
    "                    name_color = color.get_text().strip()\n",
    "                    result_dict = copy.deepcopy(result)\n",
    "                    result_dict['color']=name_color\n",
    "                    result_json.append(result_dict)\n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chromedriver.exe' if (platform.system() == 'Windows') else '/Users/jg/Desktop/develop/DataTeam/DataProcessing/product/crawling/chromedriver';\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_home = 'http://www.eglips.com'\n",
    "url_products = 'http://www.eglips.com/shop/shopbrand.html?type=P&xcode=003'\n",
    "driver.get(url_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 page crawling start!\n",
      "2 page crawling start!\n",
      "3 page crawling start!\n",
      "4 page crawling start!\n",
      "5 page crawling start!\n",
      "6 page crawling start!\n"
     ]
    }
   ],
   "source": [
    "result_json = crawlingStart(driver, url_home)\n",
    "driver.get(url_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.dumps(result_json,ensure_ascii=False, indent='\\t')\n",
    "\n",
    "writeJSON(output, output_name = 'eglips.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
